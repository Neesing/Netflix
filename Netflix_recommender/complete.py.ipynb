{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from time import time\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRatingDataSet(sc, path):\n",
    "    print(\"Loading Rating Data \")\n",
    "\n",
    "    cleanedRatingData  = sc.textFile(path)\n",
    "    headerR = cleanedRatingData.take(1)[0]\n",
    "    ratingsData = cleanedRatingData.filter(lambda line: line!= headerR)\\\n",
    "        .map(lambda line: line.split(\",\")).map(lambda tokens : (tokens[0], tokens[1], tokens[2])).cache()\n",
    "\n",
    "    print(ratingsData.take(4))\n",
    "\n",
    "    print(\"Loading Rating Data Completed\")\n",
    "\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMovieDataSet(sc, path):\n",
    "    print(\"Loading Movie Data \")\n",
    "\n",
    "    cleanedMovieData = sc.textFile(path)\n",
    "    headerM = cleanedMovieData.take(1)[0]\n",
    "\n",
    "    moviesData = cleanedMovieData.filter(lambda line: line != headerM)\\\n",
    "        .map(lambda line: line.split(\",\")).map(lambda tokens : (tokens[0], tokens[2])).cache()\n",
    "\n",
    "    print(\"Loading Movie Data Completed\")\n",
    "\n",
    "    return moviesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingTestingRDD(ratingData):    \n",
    "    trainingRDD, validationRDD, testRDD = ratingData.randomSplit([6, 2, 2])\n",
    "    predictValidationRDD = validationRDD.map(lambda x: (x[0], x[1]))\n",
    "    predictTestRDD = testRDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "    return trainingRDD, validationRDD, testRDD, predictValidationRDD, predictTestRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBestRank(trainingRDD, predictValidationRDD, validationRDD, seed, iterations, regularization_parameter):\n",
    "    rankList = [4, 8, 12]\n",
    "    errors = [0, 0, 0]\n",
    "    err = 0\n",
    "    min_error = float('inf')\n",
    "    bestRank = -1\n",
    "\n",
    "    print (\"Loop Starts\")\n",
    "    for r in rankList:\n",
    "        learningModel = ALS.train(trainingRDD, r, seed=seed, iterations=iterations,lambda_=regularization_parameter)\n",
    "        predictions = learningModel.predictAll(predictValidationRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "        ratePrediction = validationRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    \n",
    "        error = math.sqrt(ratePrediction.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "        errors[err] = error\n",
    "        err += 1\n",
    "        print ('Rank %s the RMSE is %s' % (r, error))\n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            bestRank = r\n",
    "\n",
    "    print ('The best model was trained with rank %s' % bestRank)\n",
    "\n",
    "    predictions.take(3)\n",
    "\n",
    "    ratePrediction.take(3)\n",
    "\n",
    "    print (\"Loop Ends\")\n",
    "\n",
    "    return bestRank\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestData(trainingRDD, predictTestRDD, testRDD, bestRank, seed, iterations, regularization_parameter):\n",
    "\n",
    "    model = ALS.train(trainingRDD, bestRank, seed=seed, iterations=iterations, lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(predictTestRDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    ratePrediction = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(ratePrediction.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "    print ('For testing data the RMSE is %s' % (error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainigTestingFullRatingData(fullRatingData, seed, iterations, regularization_parameter, bestRank):\n",
    "\n",
    "    print (\" Full Rating Data Set Evaluation\")\n",
    "\n",
    "    trainingRDD, testRDD = fullRatingData.randomSplit([7, 3])\n",
    "    actualModel = ALS.train(trainingRDD, bestRank, seed=seed, \n",
    "                           iterations=iterations, lambda_=regularization_parameter)\n",
    "\n",
    "    predictTestRdd = testRDD.map(lambda x: (x[0], x[1]))\n",
    "    predictions = actualModel.predictAll(predictTestRdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    ratesPred = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(ratesPred.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "    print ('RMSE is %s' % (error))\n",
    "\n",
    "    return trainingRDD, testRDD, predictTestRDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRatingData(sc, path):   \n",
    "    cleanedData =  sc.textFile('s3://project-test-n/cleaned/output-rating.csv')\n",
    "    headerR = cleanedData.take(1)[0]\n",
    "    fullRatingData = cleanedData.filter(lambda line: line!=headerR)\\\n",
    "                                .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "\n",
    "    return fullRatingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMovieData(sc, path):                                          \n",
    "    cleanedData = sc.textFile('s3://project-test-n/raw-data/movie_titles.csv')\n",
    "    headerM = cleanedData.take(1)[0]\n",
    "    movieData = cleanedData.filter(lambda line: line!=headerM)\\\n",
    "                                .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "\n",
    "    movieTitle = movieData.map(lambda x: (int(x[0]),x[1]))\n",
    "\n",
    "    return movieData, movieTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverages(idRating):\n",
    "    n = len(idRating[1])\n",
    "    return idRating[0], (n, float(sum(x for x in idRating[1]))/n)\n",
    "\n",
    "\n",
    "def loadRDDForOperations(fullRatingData):\n",
    "    movieIDRatingsRDD = (fullRatingData.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "    movieIDAvgRatingsRDD = movieIDRatingsRDD.map(getAverages)\n",
    "    movieRatingCountsRDD = movieIDAvgRatingsRDD.map(lambda x: (x[0], x[1][0]))\n",
    "\n",
    "    return movieRatingCountsRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettestData(path):\n",
    "\n",
    "    with open(path) as f:\n",
    "       new_user_ratings = [tuple(map(int, i.split(','))) for i in f]\n",
    "    print(new_user_ratings)\n",
    "\n",
    "    return new_user_ratings\n",
    "\n",
    "\n",
    "def createNewModel(completeRatingData, seed, iterations, regularization_parameter, bestRank):\n",
    "\n",
    "    t0 = time()\n",
    "    model = ALS.train(completeRatingData, bestRank, seed=seed, \n",
    "                              iterations=iterations, lambda_=regularization_parameter)\n",
    "    tt = time() - t0\n",
    "\n",
    "    print (\"Training time %s seconds\" % round(tt,3))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformAndPredictTopMovies(userRecommendationRDD, movieRatingCountRDD, movieTitle):\n",
    "    recommendationRatingData = userRecommendationRDD.map(lambda x: (x.product, x.rating))\n",
    "    ratingtitleCountRDD = \\\n",
    "        recommendationRatingData.join(movieTitle).join(movieRatingCountRDD)\n",
    "    ratingtitleCountRDD.take(3)\n",
    "\n",
    "    ratingtitleCountRDD = \\\n",
    "        ratingtitleCountRDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
    "    \n",
    "    topMovies = ratingtitleCountRDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "    print ('Movies Top :\\n%s' % '\\n'.join(map(str, topMovies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process of Learning Starts\n",
      "Loading Rating Data \n",
      "[('1', '1488844', '3'), ('1', '822109', '5'), ('1', '885013', '4'), ('1', '30878', '4')]\n",
      "Loading Rating Data Completed\n",
      "Loading Movie Data \n",
      "Loading Movie Data Completed\n",
      "Loop Starts\n",
      "Rank 4 the RMSE is 2.609759955526178\n",
      "Rank 8 the RMSE is 2.13436244193685\n",
      "Rank 12 the RMSE is 2.205045023470821\n",
      "The best model was trained with rank 8\n",
      "Loop Ends\n",
      "For testing data the RMSE is 2.1477283331059653\n",
      "Implementation\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o909.partitions.\n: java.io.IOException: No FileSystem for scheme: s3\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b1ad09079c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Implementation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mfullRatingData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadRatingData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Rating count  %s  in the  dataset\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfullRatingData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e8411072fbcf>\u001b[0m in \u001b[0;36mloadRatingData\u001b[1;34m(sc, path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloadRatingData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcleanedData\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's3://project-test-n/cleaned/output-rating.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mheaderR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleanedData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mfullRatingData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleanedData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mheaderR\u001b[0m\u001b[1;33m)\u001b[0m                                \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1311\u001b[0m         \"\"\"\n\u001b[0;32m   1312\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1313\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1314\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \"\"\"\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o909.partitions.\n: java.io.IOException: No FileSystem for scheme: s3\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print ('Process of Learning Starts')\n",
    "\n",
    "    conf = SparkConf().setAppName(\"NetFlix Prediction\").set(\"spark.executor.memory\", \"12g\").set(\"spark.driver.memory\", \"12g\")\n",
    "    sc = SparkContext()\n",
    "   \n",
    "    #to run it on AWS\n",
    "    rating_data_path = 's3://project-test-n/rank/output1.csv'\n",
    "    movie_data_path = 's3://project-test-n/movie_titles.csv'\n",
    "    complete_data_path = 's3://project-test-n/cleaned/output-rating.csv'\n",
    "    test_path = '/tmp/netflix-test.txt'\n",
    "\n",
    "    \n",
    "    #to run it locally\n",
    "    #rating_data_path = 'C:/Users/User/Netflix_dataset/sample_data.csv'\n",
    "    #movie_data_path = 'C:/Users/User/Netflix_dataset/movie_titles.csv'\n",
    "    #complete_data_path = 'C:/Users/User/Netflix_dataset/sample_data2.csv'\n",
    "\n",
    "    #test_path = 'C:/Users/User/Netflix_dataset/netflix-test.txt'\n",
    "\n",
    "    seed = 5\n",
    "    iterations = 10\n",
    "    regularization_parameter = 0.1\n",
    "    new_user_ID = 0\n",
    "\n",
    "    ratingData = createRatingDataSet(sc, rating_data_path)\n",
    "    movieData =  createMovieDataSet(sc, movie_data_path)\n",
    "\n",
    "    trainingRDD, validationRDD, testingRDD, predictValidationRDD, predictTestRDD = createTrainingTestingRDD(ratingData)\n",
    "\n",
    "    bestRank = calculateBestRank(trainingRDD, predictValidationRDD, validationRDD, seed, iterations, regularization_parameter)\n",
    "\n",
    "    predictTestData(trainingRDD, predictTestRDD, testingRDD, bestRank, seed, iterations, regularization_parameter)\n",
    "\n",
    "    print (\"Implementation\")\n",
    "\n",
    "    fullRatingData = loadRatingData(sc, complete_data_path)\n",
    "\n",
    "    print (\"Rating count  %s  in the  dataset\" % (fullRatingData.count()))\n",
    "\n",
    "    comtrainingRDD, comtestRDD, compredictTestRDD = createTrainigTestingFullRatingData(fullRatingData, seed, iterations, regularization_parameter, bestRank)\n",
    "\n",
    "    completeMovieData, movieTitle = loadMovieData(sc, movie_data_path)\n",
    "\n",
    "    print (\"There are %s movies in the complete dataset\" % (completeMovieData.count()))\n",
    "\n",
    "    testUserRating = gettestData(test_path)\n",
    "    testUserRatingRDD = sc.parallelize(testUserRating)\n",
    "    print ('New user ratings: %s' % testUserRatingRDD.take(10))\n",
    "\n",
    "    completeRatingData = fullRatingData.union(testUserRatingRDD)\n",
    "\n",
    "    model = createNewModel(completeRatingData, seed, iterations, regularization_parameter, bestRank)\n",
    "\n",
    "    testUserRatingId = map(lambda x: x[1], testUserRating)\n",
    "\n",
    "    UnratedMovieId = (completeMovieData.filter(lambda x: x[0] not in testUserRatingId).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "    userRecommendationRDD = model.predictAll(UnratedMovieId)\n",
    "\n",
    "    completeMovieTitle = movieData.map(lambda x: (int(x[0]),x[1]))\n",
    "\n",
    "    movieRatingCountsRDD = loadRDDForOperations(fullRatingData)\n",
    "\n",
    "    transformAndPredictTopMovies(userRecommendationRDD, movieRatingCountsRDD, movieTitle)\n",
    "\n",
    "    my_movie = sc.parallelize([(0, 500)]) \n",
    "    individualMovieRatingRDD = model.predictAll(UnratedMovieId)\n",
    "    print(individualMovieRatingRDD.take(2))\n",
    "\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
